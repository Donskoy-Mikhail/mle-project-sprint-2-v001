# mle-template-case-sprint2

Добро пожаловать в репозиторий-шаблон Практикума для проекта 2 спринта. Ваша цель — улучшить ключевые метрики модели для предсказания стоимости квартир Яндекс Недвижимости.

Полное описание проекта хранится в уроке «Проект. Улучшение baseline-модели» на учебной платформе.

Здесь укажите имя вашего бакета: s3-student-mle-20241021-f2a23d930e

Этап 1:
Для запуска Mlflow используется скрипт run_mlflow_server.sh 
da2610e6cfe2414cb55c5fd0b8a0cb0f -  Run ID
19 - Experiment ID 
sprint2_appartment_price

Этап 2:
Во время EDA я проверял наличие пропусков в данных, распределение признаков и корреляция между признаками и целевой переменной

Все числовые признаки несмотря на удаление выбросов имеют тяжелые хвосты

Таргет наиболее "приближен" к нормальному распределению но тоже с тяжелыми хвостами и скосом в лево

Корреляционный анализ показывает что наиболее сильные признаки это признаки которые непосредственно касаются размера квартиры kitchen_area living_area ceiling_height total_area

Этап 3:

Run ID:48a293e2c680419cad07c3e42b2cb9a4

Я вывел дполнительные признаки такие как - ratio_living_area ratio_kitchen_area и mean_flats_area

Далее для числовых признаков провел бинаризацию и также создал на их основе признаки полиномиальные

Кроме того создал дополнительные признаки используя эти трансформации на числовыми признаками ("1/", "exp", "log","sin", "cos", "exp-")

Этап 4:

Run ID:02ad4854790f4996bf5eede9a36ca76f

На этом этапе я проверил отбор признаков используя линейную регрессию из-за ограничения вычислительных ресурсов
и методы Sequential Forward Selection, Sequential Backward  Selection

В итоге я отобрал 30 признаков из метода Sequential Forward Selection. Остановился на таком количестве поскольку
большее количество признаков не имело смысла поскольку приближаясь к 30 рост качества вышел на плато

Далее обученная модель на 30 признаках показала легкий рост на 0.3 процентных пункта и ускорение обучения модели

Этап 5:

Run ID:7d6d1670d77d4a6384cf6d73e57ab7c3

На этом этапе я использовал два метода подбора гиперпараметров а именно Random Search, Bayesian Optimization

Я выбрал в итоге параметры что подобрал метод Bayesian Optimization
поскольку он улучшал метрику mape сильнее чем Random Search 14% против 16%

Итоговая модель имеет более высокие метрику чем на прошлом этапе 16.3% против 20.2%